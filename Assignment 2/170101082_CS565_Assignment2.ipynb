{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "170101082_CS565_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8dyMX5SDDGG"
      },
      "source": [
        "# **CS565 - Intelligent Systems and Interfaces - Assignment 2**\n",
        "#### **Lavish Gulati - 170101082**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVrxe8ZbI7J8",
        "outputId": "e514ecb2-14cf-47d9-e087-02fc51b7f159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Read corpus from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import codecs\n",
        "eng_text = codecs.open('/content/drive/My Drive/CS565/Assignment 2/en_wiki.txt', 'r').read()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Vh0sPhiye_",
        "outputId": "1746afcd-7382-4fb7-a75d-e1cafceb0476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Importing required libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from random import shuffle, uniform\n",
        "import math\n",
        "import statistics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glapkWglhfVO"
      },
      "source": [
        "### 2.1 - N-gram language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idWvbc6zhsdK"
      },
      "source": [
        "#### 0 - Data Preprocessing and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDDjaUJPhyv4"
      },
      "source": [
        "# Fraction subset of corpus taken\n",
        "SMALL_RATIO = 1\n",
        "eng_text = eng_text[0:int(SMALL_RATIO*len(eng_text))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzYEhTJWh9xj",
        "outputId": "01a22197-2b6c-4389-abc4-9506dd11f2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Sentence segmentation\n",
        "sentences = []\n",
        "sentences = sent_tokenize(eng_text)\n",
        "print('Number of sentences in corpus:', len(sentences))\n",
        "print('Sample data:', sentences[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in corpus: 761582\n",
            "Sample data: ['The word \"atom\" was coined by ancient Greek philosophers.', 'However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation.', 'As a result, their views on what atoms look like and how they behave were incorrect.', 'They also could not convince everybody, so atomism was but one of a number of competing theories on the nature of matter.', 'It was not until the 19th century that the idea was embraced and refined by scientists, when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-lGtqd2iFQE",
        "outputId": "2c41a39e-270a-4602-a974-07e4569425d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Word tokenization of each sentence\n",
        "word_tokens = []\n",
        "word_count = 0\n",
        "for sentence in sentences:\n",
        "  words = word_tokenize(sentence)\n",
        "  word_count += len(words)\n",
        "  word_tokens.append(words)\n",
        "print('Number of words in corpus:', word_count)\n",
        "print('Sample data:', word_tokens[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in corpus: 19602594\n",
            "Sample data: [['The', 'word', '``', 'atom', \"''\", 'was', 'coined', 'by', 'ancient', 'Greek', 'philosophers', '.'], ['However', ',', 'these', 'ideas', 'were', 'founded', 'in', 'philosophical', 'and', 'theological', 'reasoning', 'rather', 'than', 'evidence', 'and', 'experimentation', '.'], ['As', 'a', 'result', ',', 'their', 'views', 'on', 'what', 'atoms', 'look', 'like', 'and', 'how', 'they', 'behave', 'were', 'incorrect', '.'], ['They', 'also', 'could', 'not', 'convince', 'everybody', ',', 'so', 'atomism', 'was', 'but', 'one', 'of', 'a', 'number', 'of', 'competing', 'theories', 'on', 'the', 'nature', 'of', 'matter', '.'], ['It', 'was', 'not', 'until', 'the', '19th', 'century', 'that', 'the', 'idea', 'was', 'embraced', 'and', 'refined', 'by', 'scientists', ',', 'when', 'the', 'blossoming', 'science', 'of', 'chemistry', 'produced', 'discoveries', 'that', 'only', 'the', 'concept', 'of', 'atoms', 'could', 'explain', '.'], ['In', 'the', 'early', '1800s', ',', 'John', 'Dalton', 'used', 'the', 'concept', 'of', 'atoms', 'to', 'explain', 'why', 'elements', 'always', 'react', 'in', 'ratios', 'of', 'small', 'whole', 'numbers', '(', 'the', 'law', 'of', 'multiple', 'proportions', ')', '.'], ['For', 'instance', ',', 'there', 'are', 'two', 'types', 'of', 'tin', 'oxide', ':', 'one', 'is', '88.1', '%', 'tin', 'and', '11.9', '%', 'oxygen', 'and', 'the', 'other', 'is', '78.7', '%', 'tin', 'and', '21.3', '%', 'oxygen', '(', 'tin', '(', 'II', ')', 'oxide', 'and', 'tin', 'dioxide', 'respectively', ')', '.'], ['This', 'means', 'that', '100g', 'of', 'tin', 'will', 'combine', 'either', 'with', '13.5g', 'or', '27g', 'of', 'oxygen', '.'], ['13.5', 'and', '27', 'form', 'a', 'ratio', 'of', '1:2', ',', 'a', 'ratio', 'of', 'small', 'whole', 'numbers', '.'], ['This', 'common', 'pattern', 'in', 'chemistry', 'suggested', 'to', 'Dalton', 'that', 'elements', 'react', 'in', 'whole', 'number', 'multiples', 'of', 'discrete', 'units—in', 'other', 'words', ',', 'atoms', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6GjWcvyithe",
        "outputId": "8084859c-8d3e-4fd6-d2c7-e71f7e8756a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Shuffle the sentences\n",
        "shuffle(word_tokens)\n",
        "print(word_tokens[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['There', 'were', '244', 'households', 'out', 'of', 'which', '28.7', '%', 'had', 'children', 'under', 'the', 'age', 'of', '18', 'living', 'with', 'them', ',', '54.5', '%', 'were', 'married', 'couples', 'living', 'together', ',', '14.8', '%', 'had', 'a', 'female', 'householder', 'with', 'no', 'husband', 'present', ',', 'and', '27.9', '%', 'were', 'non-families', '.'], ['The', 'city', 'was', 'named', 'after', 'Mora', ',', 'Sweden', '.'], ['In', 'the', 'township', 'the', 'population', 'was', 'spread', 'out', 'with', '25.7', '%', 'under', 'the', 'age', 'of', '18', ',', '8.6', '%', 'from', '18', 'to', '24', ',', '20.0', '%', 'from', '25', 'to', '44', ',', '37.1', '%', 'from', '45', 'to', '64', ',', 'and', '8.6', '%', 'who', 'were', '65', 'years', 'of', 'age', 'or', 'older', '.'], ['Most', 'of', 'these', 'have', 'been', 'public', 'schools', 'of', 'The', 'Jefferson', 'County', 'School', 'System', 'which', 'was', 'founded', 'in', '1898', '.'], ['As', 'of', 'January', '2014', ',', 'Southwest', 'Airlines', ',', 'including', 'its', 'subsidiary', 'AirTran', 'Airways', ',', 'represents', 'approximately', '71', '%', 'of', 'passengers', 'followed', 'by', 'Delta', 'Air', 'Lines', 'at', '8', '%', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZeFgO2byK8X",
        "outputId": "f51cf2fe-a9d7-40c3-b2d6-126067e5984e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(word_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "761582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcg8zgk0l-M1"
      },
      "source": [
        "# Split the sentences into 90% train set and 10% \n",
        "train_set = word_tokens[0:int(0.9*len(word_tokens))]\n",
        "test_set = word_tokens[int(0.9*len(word_tokens)):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JspOeH9nnxWO"
      },
      "source": [
        "#### 1 - Trigram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA-gBOaPnvF8"
      },
      "source": [
        "# Extract frequency of each ngram from the training set \n",
        "def extractNgramFreq(train_set):\n",
        "  unigram_freq = {}\n",
        "  bigram_freq = {}\n",
        "  trigram_freq = {}\n",
        "\n",
        "  for sentence in train_set:\n",
        "    for i in range(len(sentence)):\n",
        "      unigram = sentence[i]\n",
        "      if unigram in unigram_freq:\n",
        "        unigram_freq[unigram] += 1\n",
        "      else:\n",
        "        unigram_freq[unigram] = 1\n",
        "    \n",
        "    for i in range(len(sentence)-1):\n",
        "      bigram = (sentence[i], sentence[i+1])\n",
        "      if bigram in bigram_freq:\n",
        "        bigram_freq[bigram] += 1\n",
        "      else:\n",
        "        bigram_freq[bigram] = 1\n",
        "    \n",
        "    for i in range(len(sentence)-2):\n",
        "      trigram = (sentence[i], sentence[i+1], sentence[i+2])\n",
        "      if trigram in trigram_freq:\n",
        "        trigram_freq[trigram] += 1\n",
        "      else:\n",
        "        trigram_freq[trigram] = 1\n",
        "  \n",
        "  return unigram_freq, bigram_freq, trigram_freq\n",
        "\n",
        "# Get probability of each unigram based on frequency and total frequency\n",
        "def getUnigramProb(unigram_freq):\n",
        "  count = 0\n",
        "  for unigram in unigram_freq:\n",
        "    count += unigram_freq[unigram]\n",
        "  \n",
        "  unigram_prob = {}\n",
        "  for unigram in unigram_freq:\n",
        "    unigram_prob[unigram] = unigram_freq[unigram]/count\n",
        "  \n",
        "  return unigram_prob\n",
        "\n",
        "\n",
        "# Get probability of each bigram based on frequency and total frequency\n",
        "def getBigramProb(unigram_freq, bigram_freq):\n",
        "  bigram_prob = {}\n",
        "  for bigram in bigram_freq:\n",
        "    bigram_prob[bigram] = bigram_freq[bigram]/unigram_freq[bigram[0]]\n",
        "  \n",
        "  return bigram_prob\n",
        "\n",
        "\n",
        "# Get probability of each trigram based on frequency and total frequency\n",
        "def getTrigramProb(bigram_freq, trigram_freq):\n",
        "  trigram_prob = {}\n",
        "  for trigram in trigram_freq:\n",
        "    trigram_prob[trigram] = trigram_freq[trigram]/bigram_freq[(trigram[0], trigram[1])]\n",
        "  \n",
        "  return trigram_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD8P_KdVmzUT"
      },
      "source": [
        "##### Discounting method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqAYa0H5un10"
      },
      "source": [
        "# Get discounted trigrams based on trigram and bigram frequency\n",
        "def trigramsDiscounting(trigram_freq, bigram_freq, beta):\n",
        "  discounted_trigrams = {}\n",
        "  for trigram in trigram_freq:\n",
        "    count = trigram_freq[trigram] - beta\n",
        "    discounted_trigrams[trigram] = count / bigram_freq[(trigram[0], trigram[1])]\n",
        "  \n",
        "  return discounted_trigrams\n",
        "\n",
        "# Calculate perplexity of the discounted trigrams \n",
        "def getDiscountingPerplexity(test_set, test_trigram_freq, discounted_trigrams):\n",
        "  entropy = 0\n",
        "  count = 0\n",
        "  test_count = 0\n",
        "  for trigram in test_trigram_freq:\n",
        "    count += test_trigram_freq[trigram]\n",
        "  \n",
        "  for sentence in test_set:\n",
        "    for i in range(len(sentence)-2):\n",
        "      trigram = (sentence[i], sentence[i+1], sentence[i+2])\n",
        "      if trigram in discounted_trigrams:\n",
        "        prob = discounted_trigrams[trigram]\n",
        "      else:\n",
        "        prob = test_trigram_freq[trigram]/count\n",
        "      \n",
        "      if prob > 0:\n",
        "        entropy += math.log(prob, 2)\n",
        "      \n",
        "      test_count += 1\n",
        "  \n",
        "  entropy = (-1*entropy)/test_count\n",
        "  perplexity = math.pow(2, entropy)\n",
        "  return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggdsNxcImoQY"
      },
      "source": [
        "# Discounting smoothing of trigrams\n",
        "def discounting(train_set, test_set):\n",
        "  unigram_freq, bigram_freq, trigram_freq = extractNgramFreq(train_set)\n",
        "  unigram_prob = getUnigramProb(unigram_freq)\n",
        "  bigram_prob = getBigramProb(unigram_freq, bigram_freq)\n",
        "  trigram_prob = getTrigramProb(bigram_freq, trigram_freq)\n",
        "\n",
        "  test_unigram_freq, test_bigram_freq, test_trigram_freq = extractNgramFreq(test_set)\n",
        "\n",
        "  beta = round(uniform(0.6,0.7), 2)\n",
        "  discounted_trigrams = trigramsDiscounting(trigram_freq, bigram_freq, beta)\n",
        "  perplexity = getDiscountingPerplexity(test_set, test_trigram_freq, discounted_trigrams)\n",
        "  return beta, perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67y6vnJS39w"
      },
      "source": [
        "##### Interpolation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04hpC2gbTRk9"
      },
      "source": [
        "# Calculate lambda set for linear interpolation smoothing\n",
        "def getLambda(test_set, epsilon, unigram_prob, bigram_prob, trigram_prob):\n",
        "  lambda_set = [0.2, 0.3, 0.5]\n",
        "  while True:\n",
        "    exp_count = [0, 0, 0]\n",
        "    for sentence in test_set:\n",
        "      for i in range(len(sentence)-2):\n",
        "        unigram = sentence[i]\n",
        "        bigram = (sentence[i], sentence[i+1])\n",
        "        trigram = (sentence[i], sentence[i+1], sentence[i+2])\n",
        "        \n",
        "        prob1 = unigram_prob[unigram] if unigram in unigram_prob else 0\n",
        "        prob2 = bigram_prob[bigram] if bigram in bigram_prob else 0\n",
        "        prob3 = trigram_prob[trigram] if trigram in trigram_prob else 0\n",
        "\n",
        "        prob = lambda_set[0]*prob1 + lambda_set[1]*prob2 + lambda_set[2]*prob3\n",
        "\n",
        "        if prob > 0:\n",
        "          exp_count[0] += (lambda_set[0]*prob1)/prob\n",
        "          exp_count[1] += (lambda_set[1]*prob2)/prob\n",
        "          exp_count[2] += (lambda_set[2]*prob3)/prob\n",
        "    \n",
        "    exp_sum = sum(exp_count)\n",
        "    if exp_sum == 0:\n",
        "      return lambda_set\n",
        "    \n",
        "    new_lambda_set = [exp/exp_sum for exp in exp_count]\n",
        "\n",
        "    flag = True\n",
        "    for i in range(0, 3):\n",
        "      diff = abs(new_lambda_set[i]-lambda_set[i])\n",
        "      if diff > epsilon:\n",
        "        flag = False\n",
        "        break\n",
        "    \n",
        "    if flag:\n",
        "      return lambda_set\n",
        "    else:\n",
        "      lambda_set = new_lambda_set\n",
        "\n",
        "\n",
        "# Calculate perplexity for interpolation\n",
        "def getInterpolationPerplexity(test_set, lambda_set, unigram_prob, bigram_prob, trigram_prob):\n",
        "  entropy = 0\n",
        "  test_count = 0\n",
        "  for sentence in test_set:\n",
        "    for i in range(len(sentence)-2):\n",
        "      unigram = sentence[i]\n",
        "      bigram = (sentence[i], sentence[i+1])\n",
        "      trigram = (sentence[i], sentence[i+1], sentence[i+2])\n",
        "        \n",
        "      prob1 = unigram_prob[unigram] if unigram in unigram_prob else 0\n",
        "      prob2 = bigram_prob[bigram] if bigram in bigram_prob else 0\n",
        "      prob3 = trigram_prob[trigram] if trigram in trigram_prob else 0\n",
        "\n",
        "      prob = lambda_set[0]*prob1 + lambda_set[1]*prob2 + lambda_set[2]*prob3\n",
        "      if prob > 0:\n",
        "        entropy += math.log(prob, 2)\n",
        "      \n",
        "      test_count += 1\n",
        "  \n",
        "  entropy = (-1*entropy)/test_count\n",
        "  perplexity = math.pow(2, entropy)\n",
        "  return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKLl9MNOS8Ax"
      },
      "source": [
        "# Linear Interpolation smoothing\n",
        "def interpolation(train_set, test_set):\n",
        "  unigram_freq, bigram_freq, trigram_freq = extractNgramFreq(train_set)\n",
        "  unigram_prob = getUnigramProb(unigram_freq)\n",
        "  bigram_prob = getBigramProb(unigram_freq, bigram_freq)\n",
        "  trigram_prob = getTrigramProb(bigram_freq, trigram_freq)\n",
        "\n",
        "  lambda_set = getLambda(test_set, 0.005, unigram_prob, bigram_prob, trigram_prob)\n",
        "  perplexity = getInterpolationPerplexity(test_set, lambda_set, unigram_prob, bigram_prob, trigram_prob)\n",
        "  return lambda_set, perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7qDwAcDjs1z"
      },
      "source": [
        "#### 2 - Splitting training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWeaOt7fj5Vr"
      },
      "source": [
        "# Split original training set into training set and validation set\n",
        "def splitTrainSet(train_set):\n",
        "  shuffle(train_set)\n",
        "  new_train_set = train_set[0:int(len(train_set)*0.9)]\n",
        "  validation_set = train_set[int(len(train_set)*0.9):]\n",
        "  return new_train_set, validation_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHiZbOy3kbz_"
      },
      "source": [
        "ts1, vs1 = splitTrainSet(train_set)\n",
        "ts2, vs2 = splitTrainSet(train_set)\n",
        "ts3, vs3 = splitTrainSet(train_set)\n",
        "ts4, vs4 = splitTrainSet(train_set)\n",
        "ts5, vs5 = splitTrainSet(train_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OYR7QNfkqQz"
      },
      "source": [
        "#### 3 - Performance on Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdUW6BLFk02o"
      },
      "source": [
        "##### Discounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXL7dP_ik4wF",
        "outputId": "088f2f13-c56b-46bb-b2e6-b827799701e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate discounting beta and perplexity for all validation sets\n",
        "beta1, perp1 = discounting(ts1, vs1)\n",
        "beta2, perp2 = discounting(ts2, vs2)\n",
        "beta3, perp3 = discounting(ts3, vs3)\n",
        "beta4, perp4 = discounting(ts4, vs4)\n",
        "beta5, perp5 = discounting(ts5, vs5)\n",
        "beta = [beta1, beta2, beta3, beta4, beta5]\n",
        "perp = [perp1, perp2, perp3, perp4, perp5]\n",
        "\n",
        "print('Discounting performance on validation set')\n",
        "for i in range(0, 5):\n",
        "  print('Set', i+1, '- beta:', beta[i], ', perplexity:', perp[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discounting performance on validation set\n",
            "Set 1 - beta: 0.61 , perplexity: 385.98464393175266\n",
            "Set 2 - beta: 0.61 , perplexity: 335.20801152217865\n",
            "Set 3 - beta: 0.66 , perplexity: 362.5238493150579\n",
            "Set 4 - beta: 0.64 , perplexity: 378.2673871942632\n",
            "Set 5 - beta: 0.62 , perplexity: 346.0446900001597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzA648HJmLbj"
      },
      "source": [
        "##### Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLF_c4welvPv",
        "outputId": "d97d385a-a190-4e07-d0f4-291a458560d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate interpolation lambda and perplexity for all validation sets\n",
        "ls1, perp1 = interpolation(ts1, vs1)\n",
        "ls2, perp2 = interpolation(ts2, vs2)\n",
        "ls3, perp3 = interpolation(ts3, vs3)\n",
        "ls4, perp4 = interpolation(ts4, vs4)\n",
        "ls5, perp5 = interpolation(ts5, vs5)\n",
        "ls = [ls1, ls2, ls3, ls4, ls5]\n",
        "perp = [perp1, perp2, perp3, perp4, perp5]\n",
        "\n",
        "print('Interpolation performance on validation set')\n",
        "for i in range(0, 5):\n",
        "  print('Set', i+1, '- lambda set:', ls[i], ', perplexity:', perp[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation performance on validation set\n",
            "Set 1 - lambda set: [0.3277119588920003, 0.39650946907696855, 0.2757785720310312] , perplexity: 91.22769316275311\n",
            "Set 2 - lambda set: [0.3295335779794422, 0.3963021384365197, 0.2741642835840382] , perplexity: 92.42521601363109\n",
            "Set 3 - lambda set: [0.32792929962145495, 0.396991874037794, 0.2750788263407509] , perplexity: 91.77695155824924\n",
            "Set 4 - lambda set: [0.3307374088539322, 0.39644097666968764, 0.2728216144763801] , perplexity: 93.04468779624617\n",
            "Set 5 - lambda set: [0.3294072328392538, 0.3964805020221355, 0.27411226513861076] , perplexity: 92.3473527959448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DoZHk2pmfuv"
      },
      "source": [
        "#### 4 - Performance on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnoQj7Otmquz"
      },
      "source": [
        "##### Discounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1MgE8pQmvH0",
        "outputId": "2369ccd6-1d25-4674-e8a2-92a337263423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate discounting beta and perplexity for all test sets and compute variance\n",
        "beta1, perp1 = discounting(ts1, test_set)\n",
        "beta2, perp2 = discounting(ts2, test_set)\n",
        "beta3, perp3 = discounting(ts3, test_set)\n",
        "beta4, perp4 = discounting(ts4, test_set)\n",
        "beta5, perp5 = discounting(ts5, test_set)\n",
        "beta = [beta1, beta2, beta3, beta4, beta5]\n",
        "perp = [perp1, perp2, perp3, perp4, perp5]\n",
        "\n",
        "print('Discounting performance on test set')\n",
        "for i in range(0, 5):\n",
        "  print('Set', i+1, '- beta:', beta[i], ', perplexity:', perp[i])\n",
        "\n",
        "print(\"Variance of perplexity:\", statistics.variance(perp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discounting performance on test set\n",
            "Set 1 - beta: 0.65 , perplexity: 384.1294669832734\n",
            "Set 2 - beta: 0.61 , perplexity: 379.87093784436684\n",
            "Set 3 - beta: 0.63 , perplexity: 383.77810953043127\n",
            "Set 4 - beta: 0.64 , perplexity: 382.5792679308408\n",
            "Set 5 - beta: 0.64 , perplexity: 382.0304207051159\n",
            "Variance of perplexity: 2.8562457111186608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-ZhuKNBm-Fk"
      },
      "source": [
        "##### Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmbwCDcXm_qG",
        "outputId": "96640a18-a478-4ed8-8d18-eb4da67b724a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate interpolation lambda and perplexity for all test sets and compute variance\n",
        "ls1, perp1 = interpolation(ts1, test_set)\n",
        "ls2, perp2 = interpolation(ts2, test_set)\n",
        "ls3, perp3 = interpolation(ts3, test_set)\n",
        "ls4, perp4 = interpolation(ts4, test_set)\n",
        "ls5, perp5 = interpolation(ts5, test_set)\n",
        "ls = [ls1, ls2, ls3, ls4, ls5]\n",
        "perp = [perp1, perp2, perp3, perp4, perp5]\n",
        "\n",
        "print('Interpolation performance on test set')\n",
        "for i in range(0, 5):\n",
        "  print('Set', i+1, '- lambda set:', ls[i], ', perplexity:', perp[i])\n",
        "\n",
        "print(\"Variance of perplexity:\", statistics.variance(perp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation performance on test set\n",
            "Set 1 - lambda set: [0.3296833397486049, 0.39652110232477983, 0.27379555792661514] , perplexity: 91.99739879716303\n",
            "Set 2 - lambda set: [0.3295044469515101, 0.39658843144569367, 0.2739071216027961] , perplexity: 91.78268638383216\n",
            "Set 3 - lambda set: [0.3296018985447135, 0.39650282503347606, 0.2738952764218105] , perplexity: 91.87850940426299\n",
            "Set 4 - lambda set: [0.3296188623024507, 0.3963528913693927, 0.27402824632815653] , perplexity: 91.91424649511205\n",
            "Set 5 - lambda set: [0.3294907874206512, 0.3965074778124422, 0.27400173476690654] , perplexity: 91.8246370330346\n",
            "Variance of perplexity: 0.006872811974727759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTnKKy73nDH6"
      },
      "source": [
        "#### 5 - Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq3gK1vEw5Zr"
      },
      "source": [
        "# Get laplace probability of trigrams based on trigram and bigram frequency\n",
        "def getLaplaceProb(trigram_freq, bigram_freq):\n",
        "  k = 1\n",
        "  trigram_prob = {}\n",
        "  for trigram in trigram_freq:\n",
        "    bigram = (trigram[0], trigram[1])\n",
        "    trigram_prob[trigram] = (trigram_freq[trigram] + k) / (bigram_freq[bigram]+ k*len(trigram_freq))\n",
        "  return trigram_prob\n",
        "\n",
        "\n",
        "# Calculate perplexity of laplace smoothing\n",
        "def getLaplacePerplexity(test_set, trigram_prob):\n",
        "  entropy = 0\n",
        "  test_count = 0\n",
        "  for sentence in test_set:\n",
        "    for i in range(len(sentence)-2):\n",
        "      trigram = (sentence[i], sentence[i+1], sentence[i+2])\n",
        "      prob = trigram_prob[trigram] if trigram in trigram_prob else 0\n",
        "\n",
        "      if prob > 0:\n",
        "        entropy += math.log(prob, 2)\n",
        "\n",
        "      test_count += 1\n",
        "\n",
        "  entropy = (-1*entropy)/test_count\n",
        "  perplexity = math.pow(2, entropy)\n",
        "  return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dLBWJM1nLGm"
      },
      "source": [
        "# Laplace smoothing\n",
        "def laplace(train_set, test_set):\n",
        "  unigram_freq, bigram_freq, trigram_freq = extractNgramFreq(train_set)\n",
        "  trigram_prob = getLaplaceProb(trigram_freq, bigram_freq)\n",
        "  perplexity = getLaplacePerplexity(test_set, trigram_prob)\n",
        "  return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erZtxLgQyzEr",
        "outputId": "0f82b1b8-6d61-4204-9dac-e24cbde83b95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate laplace perplexity for all test sets and compute variance\n",
        "perp1 = laplace(ts1, test_set)\n",
        "perp2 = laplace(ts2, test_set)\n",
        "perp3 = laplace(ts3, test_set)\n",
        "perp4 = laplace(ts4, test_set)\n",
        "perp5 = laplace(ts5, test_set)\n",
        "perp = [perp1, perp2, perp3, perp4, perp5]\n",
        "\n",
        "print('Laplace performance on test set')\n",
        "for i in range(0, 5):\n",
        "  print('Set', i+1, '- perplexity:', perp[i])\n",
        "\n",
        "print(\"Variance of perplexity:\", statistics.variance(perp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Laplace performance on test set\n",
            "Set 1 - perplexity: 589.2159070170205\n",
            "Set 2 - perplexity: 589.9118492378913\n",
            "Set 3 - perplexity: 588.7354057333338\n",
            "Set 4 - perplexity: 591.6856934734319\n",
            "Set 5 - perplexity: 589.5165635032213\n",
            "Variance of perplexity: 1.2805890031643805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmtrHDb3DUa5"
      },
      "source": [
        "### 2.2 - Vector Semantics: GloVE implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx52U0krDfnn"
      },
      "source": [
        "#### 1 - GloVe embedding method implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgDAZ58PQzNR"
      },
      "source": [
        "# Import required libraries\n",
        "from collections import Counter\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "from math import log\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PRW4kDGFX_l"
      },
      "source": [
        "# Builds a vocabulary of words mapped to word ID and word frequency in the corpus\n",
        "def build_vocabulary(corpus):\n",
        "  vocabulary = Counter()\n",
        "  # Tokenize corpus into sentences\n",
        "  sentences = sent_tokenize(corpus)\n",
        "  for sentence in sentences:\n",
        "    # Tokenize each sentence into words and update the vocabulary\n",
        "    words = word_tokenize(sentence)\n",
        "    vocabulary.update(words)\n",
        "  \n",
        "  # Return vocabulary and list of sentences\n",
        "  return {word: (id, freq) for id, (word, freq) in enumerate(vocabulary.items())}, sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqCFeWotN343"
      },
      "source": [
        "# Builds the word cooccurrence matrix X for the given vocabulary. The final\n",
        "# result is a list containing tuples of (center_ID, context_ID, Xij)\n",
        "def build_cooccurrences(vocabulary, sentences, contextWindow):\n",
        "  # Let V be the size of the vocabulary\n",
        "  vocabSize = len(vocabulary)\n",
        "  result = []\n",
        "\n",
        "  # Initialize a sparse matrix for cooccurrences of size V x V\n",
        "  cooccurrences = sparse.lil_matrix((vocabSize, vocabSize), dtype=np.float64)\n",
        "\n",
        "  # Iterate over all sentences in the corpus\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    # Iterate over all words in the sentence and fetch their word IDs in a list\n",
        "    words = word_tokenize(sentence)\n",
        "    wordIds = [vocabulary[word][0] for word in words]\n",
        "\n",
        "    for i, centerId in enumerate(wordIds):\n",
        "      # Collect all context word IDs in left window of center word\n",
        "      contextIds = wordIds[max(0, i - contextWindow) : i]\n",
        "      contextLen = len(contextIds)\n",
        "\n",
        "      # Iterate over each context word in left window\n",
        "      for j, contextId in enumerate(contextIds):\n",
        "        # Distance from center word\n",
        "        distance = contextLen - j\n",
        "        \n",
        "        # Weight by inverse of distance between words\n",
        "        increment = 1.0 / float(distance)\n",
        "        \n",
        "        # Update cooccurrences of context word and center word symmetrically to\n",
        "        # handle both left window and right window cooccurrences\n",
        "        cooccurrences[centerId, contextId] += increment\n",
        "        cooccurrences[contextId, centerId] += increment\n",
        "  \n",
        "  # Build the output result from the sparse matrix\n",
        "  for i, (row, data) in enumerate(zip(cooccurrences.rows, cooccurrences.data)):\n",
        "    for data_idx, j in enumerate(row):\n",
        "      result.append((i, j, data[data_idx]))\n",
        "  \n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJQw8V-erRt4"
      },
      "source": [
        "# Runs a single iteration of adaptive gradient descent (AdaGrad) while training\n",
        "# the GloVe word embeddings. Takes input cooccurrence data, weight vectors,\n",
        "# biases and gradient histories. Returns the cost associated with\n",
        "# the given weights and updates the weights by AdaGrad\n",
        "def run_iteration(vocabulary, data, learningRate, xMax, alpha):\n",
        "  globalCost = 0\n",
        "\n",
        "  # Shuffle the data to avoid biasing of the word vectors\n",
        "  shuffle(data)\n",
        "\n",
        "  for (vMain, vContext, bMain, bContext, gradWMain, gradWContext,\n",
        "       gradBMain, gradBContext, cooccurrence) in data:\n",
        "    \n",
        "    weight = (cooccurrence / xMax) ** alpha if cooccurrence < xMax else 1\n",
        "    \n",
        "    # Compute inner component of cost function\n",
        "    costInner = (vMain.dot(vContext) + bMain[0] + bContext[0] - log(cooccurrence))\n",
        "    \n",
        "    # Compute cost function\n",
        "    cost = weight * (costInner ** 2)\n",
        "\n",
        "    # Add weighted cost to the global cost\n",
        "    globalCost += 0.5 * cost\n",
        "\n",
        "    # Compute gradients for word vectors\n",
        "    gradMain = weight * costInner * vContext\n",
        "    gradContext = weight * costInner * vMain\n",
        "\n",
        "    # Compute gradients for bias terms\n",
        "    gradBiasMain = weight * costInner\n",
        "    gradBiasContext = weight * costInner\n",
        "\n",
        "    # Perform adaptive gradient descent\n",
        "    vMain -= (learningRate * gradMain / np.sqrt(gradWMain))\n",
        "    vContext -= (learningRate * gradContext / np.sqrt(gradWContext))\n",
        "\n",
        "    bMain -= (learningRate * gradBiasMain / np.sqrt(gradBMain))\n",
        "    bContext -= (learningRate * gradBiasContext / np.sqrt(gradBContext))\n",
        "    \n",
        "    # Update squared gradient sums\n",
        "    gradWMain += np.square(gradMain)\n",
        "    gradWContext += np.square(gradContext)\n",
        "    gradBMain += gradBiasMain ** 2\n",
        "    gradBContext += gradBiasContext ** 2\n",
        "  \n",
        "  return globalCost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc4u02hDpLqt"
      },
      "source": [
        "# Train GloVe vectors given cooccurrences and vocabulary. Takes input other\n",
        "# parameters such as dimSize, iterations, learningRate, xMax and alpha. Returns\n",
        "# the computed word vector matrix W of size 2V * d\n",
        "def train_glove(vocabulary, cooccurrences, dimSize, iterations, learningRate, xMax, alpha):\n",
        "  \n",
        "  vocabSize = len(vocabulary)\n",
        "\n",
        "  # Word vector matrix of size 2V * d initialized by random values in range\n",
        "  # (-0.5, 0.5]\n",
        "  W = (np.random.rand(vocabSize * 2, dimSize) - 0.5) / float(dimSize + 1)\n",
        "\n",
        "  # Bias terms associated with each single vector initialized by random values\n",
        "  # in range (-0.5, 0.5]\n",
        "  biases = (np.random.rand(vocabSize * 2) - 0.5) / float(dimSize + 1)\n",
        "\n",
        "  # Sum of squares of all previous gradients for adaptive gradient descent\n",
        "  # (AdaGrad) initialized to 1 so that initial adaptive learning rate is equal\n",
        "  # to global learning rate\n",
        "  gradient = np.ones((vocabSize * 2, dimSize), dtype=np.float64)\n",
        "\n",
        "  # Sum of squared gradients for the bias terms\n",
        "  gradientBiases = np.ones(vocabSize * 2, dtype=np.float64)\n",
        "\n",
        "  data = [ (W[iMain], W[iContext + vocabSize],\n",
        "            biases[iMain : iMain + 1],\n",
        "            biases[iContext + vocabSize : iContext + vocabSize + 1],\n",
        "            gradient[iMain], gradient[iContext + vocabSize],\n",
        "            gradientBiases[iMain : iMain + 1],\n",
        "            gradientBiases[iContext + vocabSize : iContext + vocabSize + 1],\n",
        "            cooccurrence )\n",
        "            for iMain, iContext, cooccurrence in cooccurrences]\n",
        "  \n",
        "  # Train the word vector matrix for specific number of iterations\n",
        "  for i in tqdm(range(iterations)):\n",
        "    cost = run_iteration(vocabulary, data, learningRate, xMax, alpha)\n",
        "    print('Iteration', i, '- Cost:', cost)\n",
        "\n",
        "  # Return the word vector matrix\n",
        "  return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu5j1-x_EsXI"
      },
      "source": [
        "def glove_embeddings(corpus, contextWindow, dimSize, iterations, learningRate, xMax, alpha):\n",
        "  vocabulary, sentences = build_vocabulary(corpus)\n",
        "  cooccurrences = build_cooccurrences(vocabulary, sentences, contextWindow)\n",
        "  W = train_glove(vocabulary, cooccurrences, dimSize, iterations, learningRate, xMax, alpha)\n",
        "  return W, vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6SPkm0tDe3_",
        "outputId": "01c3e3db-6de8-4176-bc83-ecad705b6938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "CONTEXT_WINDOW = 10\n",
        "DIM_SIZE = 100\n",
        "ITERATIONS = 50\n",
        "LEARNING_RATE = 0.05\n",
        "X_MAX = 100\n",
        "ALPHA = 0.75\n",
        "SMALL_RATIO = 0.15\n",
        "CORPUS = eng_text[0:int(SMALL_RATIO*len(eng_text))]\n",
        "\n",
        "W, vocabulary = glove_embeddings(CORPUS, contextWindow=CONTEXT_WINDOW, dimSize=DIM_SIZE,\n",
        "                            iterations=ITERATIONS, learningRate=LEARNING_RATE,\n",
        "                            xMax=X_MAX, alpha=ALPHA)\n",
        "\n",
        "print('Vocabulary Size:', len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [05:05<4:09:47, 305.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 - Cost: 246414.4416228635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 2/50 [10:08<4:03:57, 304.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 - Cost: 154526.7653301233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 3/50 [15:10<3:58:04, 303.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2 - Cost: 129975.49430860451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 4/50 [20:08<3:51:40, 302.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 3 - Cost: 115975.06002828087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 5/50 [25:07<3:46:02, 301.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 4 - Cost: 106917.5988612259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 6/50 [30:00<3:39:09, 298.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 5 - Cost: 100210.47771836862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 7/50 [34:57<3:33:42, 298.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 6 - Cost: 94930.37532468761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 8/50 [40:05<3:30:42, 301.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 7 - Cost: 90562.55627817665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 9/50 [45:27<3:30:02, 307.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 8 - Cost: 86830.5861695506\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 10/50 [50:38<3:25:46, 308.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 9 - Cost: 83600.9301966498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 11/50 [55:52<3:21:37, 310.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 10 - Cost: 80745.79094217354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 12/50 [1:00:57<3:15:30, 308.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 11 - Cost: 78171.45209902308\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 13/50 [1:05:53<3:07:58, 304.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 12 - Cost: 75819.23543946167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 14/50 [1:10:49<3:01:19, 302.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 13 - Cost: 73644.20599651117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 15/50 [1:15:45<2:55:04, 300.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 14 - Cost: 71617.69289320953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 16/50 [1:20:40<2:49:14, 298.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 15 - Cost: 69710.3028750657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 17/50 [1:25:40<2:44:27, 299.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 16 - Cost: 67913.0281358426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 18/50 [1:30:55<2:42:08, 304.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 17 - Cost: 66233.67418178875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 19/50 [1:36:04<2:37:48, 305.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 18 - Cost: 64660.949307642906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 20/50 [1:41:08<2:32:31, 305.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 19 - Cost: 63164.29752962177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 21/50 [1:46:21<2:28:33, 307.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 20 - Cost: 61758.71037683993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 22/50 [1:51:19<2:22:05, 304.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 21 - Cost: 60434.02070074566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 23/50 [1:56:17<2:16:12, 302.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 22 - Cost: 59188.25196562519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 24/50 [2:01:14<2:10:26, 301.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 23 - Cost: 58001.709574166394\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 25/50 [2:06:17<2:05:40, 301.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 24 - Cost: 56879.76416359309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 26/50 [2:11:25<2:01:25, 303.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 25 - Cost: 55811.873864295165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 27/50 [2:16:24<1:55:50, 302.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 26 - Cost: 54799.311242563985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 28/50 [2:21:26<1:50:44, 302.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 27 - Cost: 53831.833512562655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 29/50 [2:26:25<1:45:20, 300.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 28 - Cost: 52911.14058270578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 30/50 [2:31:23<1:40:05, 300.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 29 - Cost: 52028.00405656768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 31/50 [2:36:22<1:34:55, 299.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 30 - Cost: 51182.89150377552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 32/50 [2:41:21<1:29:53, 299.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 31 - Cost: 50374.914319206444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 66%|██████▌   | 33/50 [2:46:20<1:24:51, 299.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 32 - Cost: 49600.20728149509\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 34/50 [2:51:17<1:19:39, 298.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 33 - Cost: 48853.89665440055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 35/50 [2:56:21<1:15:03, 300.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 34 - Cost: 48140.89954903039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 72%|███████▏  | 36/50 [3:01:21<1:10:00, 300.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 35 - Cost: 47452.756402472274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 74%|███████▍  | 37/50 [3:06:24<1:05:12, 301.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 36 - Cost: 46791.227763308445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 38/50 [3:11:31<1:00:34, 302.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 37 - Cost: 46154.14248940452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 39/50 [3:16:28<55:12, 301.13s/it]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 38 - Cost: 45539.823257775985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 40/50 [3:21:27<50:05, 300.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 39 - Cost: 44946.5896295189\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 41/50 [3:26:27<45:01, 300.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 40 - Cost: 44375.06064292489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 42/50 [3:31:25<39:56, 299.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 41 - Cost: 43822.886799995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 43/50 [3:36:22<34:52, 298.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 42 - Cost: 43288.36095228255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 44/50 [3:41:18<29:47, 297.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 43 - Cost: 42771.512271710104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 45/50 [3:46:15<24:49, 297.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 44 - Cost: 42272.99411572941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 46/50 [3:51:12<19:49, 297.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 45 - Cost: 41785.37080241512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 94%|█████████▍| 47/50 [3:56:10<14:52, 297.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 46 - Cost: 41315.10245755931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 48/50 [4:01:09<09:56, 298.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 47 - Cost: 40859.07206297673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 98%|█████████▊| 49/50 [4:06:06<04:57, 297.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 48 - Cost: 40415.34033631018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [4:11:06<00:00, 301.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 49 - Cost: 39985.61136303595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 99287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlH2vAu1N29a",
        "outputId": "0d7985c8-4893-4ffb-b4d1-fcfd09dbc127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Final GloVe word embedding is obtained by adding the center word embedding\n",
        "# and context word embedding for each word\n",
        "print(W.shape)\n",
        "embeddings = {}\n",
        "vocabSize = len(vocabulary)\n",
        "for word, (id, _) in vocabulary.items():\n",
        "  embeddings[word] = W[id]+W[id+vocabSize]\n",
        "\n",
        "# Save the embeddings in a pickle file\n",
        "with open('embeddings.pickle', 'wb') as handle:\n",
        "    pickle.dump(embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save the weights in a pickle file\n",
        "with open('weights.pickle', 'wb') as handle:\n",
        "    pickle.dump(W, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(153650, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxOuCYkuNFgi"
      },
      "source": [
        "#### 2 - Comparing Word Similarity\n",
        "Dependencies: embeddings.pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IchA4i1qel0"
      },
      "source": [
        "# Load the embeddings from pickle file\n",
        "with open('embeddings.pickle', 'rb') as handle:\n",
        "    embeddings = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaYX6SqvLhSN"
      },
      "source": [
        "# Copy the benchmark tool to the current session\n",
        "! rm -rf web\n",
        "! cp -r '/content/drive/My Drive/CS565/Assignment 2/word-embeddings-benchmarks/web' ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyZjwKPyO0hN"
      },
      "source": [
        "# Import required libraries\n",
        "from web.datasets.similarity import *\n",
        "from web.evaluate import evaluate_similarity\n",
        "from web.embeddings import fetch_GloVe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImZqQGYbNjm3"
      },
      "source": [
        "# Define benchmark tasks for word similarity comparison \n",
        "tasks = {\n",
        "    \"MEN\": fetch_MEN(),\n",
        "    \"WS353\": fetch_WS353(),\n",
        "    \"SIMLEX999\": fetch_SimLex999(),\n",
        "    \"RG65\": fetch_RG65(),\n",
        "    \"MTurk\": fetch_MTurk(),\n",
        "    \"RW\": fetch_RW()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJZF8kFXOmu4",
        "outputId": "672aa3dc-3d74-4a2d-86f1-13e653e6ae09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print sample data\n",
        "for name, data in tasks.items():\n",
        "    print(\"Sample data from {}: pair \\\"{}\\\" and \\\"{}\\\" is assigned score {}\".format(name, data.X[0][0], data.X[0][1], data.y[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample data from MEN: pair \"sun\" and \"sunlight\" is assigned score [10.]\n",
            "Sample data from WS353: pair \"love\" and \"sex\" is assigned score 6.77\n",
            "Sample data from SIMLEX999: pair \"old\" and \"new\" is assigned score 1.58\n",
            "Sample data from RG65: pair \"gem\" and \"jewel\" is assigned score 9.85\n",
            "Sample data from MTurk: pair \"episcopal\" and \"russia\" is assigned score 5.5\n",
            "Sample data from RW: pair \"squishing\" and \"squirt\" is assigned score 5.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6xEEeCXOtY2",
        "outputId": "19bea0f4-983c-4ff8-b63d-6ff7a385251d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate results using helper function\n",
        "for name, data in tasks.items():\n",
        "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(embeddings, data.X, data.y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing 803 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on MEN 0.1295455136075063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 52 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on WS353 0.20866820523498564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 94 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on SIMLEX999 0.07120656894553914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 30 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on RG65 0.1629543816722517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 125 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on MTurk 0.18482107742423962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 1743 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on RW 0.20551540968104426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj2TwD33dWdX",
        "outputId": "5d6b692c-3be0-49dc-e376-d20be26a50d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Fetch GloVe embeddings\n",
        "gloveEmbeddings = fetch_GloVe(corpus=\"twitter-27B\", dim=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File already downloaded, skipping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYJsvg0jgW05",
        "outputId": "844349ac-a476-415e-99c2-2f862539314c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate results using helper function\n",
        "for name, data in tasks.items():\n",
        "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(gloveEmbeddings, data.X, data.y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing 1 words. Will replace them with mean vector\n",
            "Missing 25 words. Will replace them with mean vector\n",
            "Missing 1 words. Will replace them with mean vector\n",
            "Missing 1 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on MEN 0.5773369776281995\n",
            "Spearman correlation of scores on WS353 0.46979381939437287\n",
            "Spearman correlation of scores on SIMLEX999 0.12221121100798378\n",
            "Spearman correlation of scores on RG65 0.6774486160113895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 1071 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on MTurk 0.5641004632190647\n",
            "Spearman correlation of scores on RW 0.23074174522387267\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}